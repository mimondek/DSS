{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando pacotes\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import Counter \n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#users=np.arange(0, 9, 1).tolist()\n",
    "users = [\n",
    "    { \"id\": 0, \"name\": \"Hero\" },\n",
    "    { \"id\": 1, \"name\": \"Dunn\" },\n",
    "    { \"id\": 2, \"name\": \"Sue\" },\n",
    "    { \"id\": 3, \"name\": \"Chi\" },\n",
    "    { \"id\": 4, \"name\": \"Thor\" },\n",
    "    { \"id\": 5, \"name\": \"Clive\" },\n",
    "    { \"id\": 6, \"name\": \"Hicks\" },\n",
    "    { \"id\": 7, \"name\": \"Devin\" },\n",
    "    { \"id\": 8, \"name\": \"Kate\" },\n",
    "    { \"id\": 9, \"name\": \"Klein\" }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "friendship_pairs=[(0,1),(0,2),(1,2),(1,3),(2,3),(3,4),(4,5),(5,6),(5,7),(6,8),(7,8),(8,9)]\n",
    "friendships = {user[\"id\"]: [] for user in users}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (0, 2)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(0,1),(0,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in friendship_pairs:\n",
    "    friendships[i].append(j)  # Add j as a friend of user i\n",
    "    friendships[j].append(i)  # Add i as a friend of user j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_friends(user):\n",
    "    return(len(user[\"friends\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_friends1(user):\n",
    "    \"\"\"How many friends does _user_ have?\"\"\"\n",
    "    return len(friendships[user[\"id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_connections=sum(number_of_friends1(user) for user in users)\n",
    "num_users=len(users)\n",
    "avg_connection=total_connections/num_users\n",
    "num_friends_by_id = [(user[\"id\"],number_of_friends1(user)) for user in users]\n",
    "total_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_friends_by_id.sort(key=lambda id_and_friends:id_and_friends[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foaf_ids_bad(user):\n",
    "    user_id = user[\"id\"]\n",
    "    return Counter(\n",
    "        foaf_id\n",
    "        for friend_id in friendships[user_id]     # For each of my friends,\n",
    "        for foaf_id in friendships[friend_id]     # find their friends\n",
    "        if foaf_id != user_id                     # who aren't me\n",
    "        and foaf_id not in friendships[user_id]   # and aren't my friends.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 2})\n",
      "Counter({1: 1, 2: 1, 6: 1, 7: 1})\n",
      "Counter({6: 2, 4: 1, 9: 1})\n"
     ]
    }
   ],
   "source": [
    "print(foaf_ids_bad(users[0])) \n",
    "print(foaf_ids_bad(users[4])) \n",
    "print(foaf_ids_bad(users[7])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "interests = [\n",
    "    (0, \"Hadoop\"), (0, \"Big Data\"), (0, \"HBase\"), (0, \"Java\"),\n",
    "    (0, \"Spark\"), (0, \"Storm\"), (0, \"Cassandra\"),\n",
    "    (1, \"NoSQL\"), (1, \"MongoDB\"), (1, \"Cassandra\"), (1, \"HBase\"),\n",
    "    (1, \"Postgres\"), (2, \"Python\"), (2, \"scikit-learn\"), (2, \"scipy\"),\n",
    "    (2, \"numpy\"), (2, \"statsmodels\"), (2, \"pandas\"), (3, \"R\"), (3, \"Python\"),\n",
    "    (3, \"statistics\"), (3, \"regression\"), (3, \"probability\"),\n",
    "    (4, \"machine learning\"), (4, \"regression\"), (4, \"decision trees\"),\n",
    "    (4, \"libsvm\"), (5, \"Python\"), (5, \"R\"), (5, \"Java\"), (5, \"C++\"),\n",
    "    (5, \"Haskell\"), (5, \"programming languages\"), (6, \"statistics\"),\n",
    "    (6, \"probability\"), (6, \"mathematics\"), (6, \"theory\"),\n",
    "    (7, \"machine learning\"), (7, \"scikit-learn\"), (7, \"Mahout\"),\n",
    "    (7, \"neural networks\"), (8, \"neural networks\"), (8, \"deep learning\"),\n",
    "    (8, \"Big Data\"), (8, \"artificial intelligence\"), (9, \"Hadoop\"),\n",
    "    (9, \"Java\"), (9, \"MapReduce\"), (9, \"Big Data\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_science_who_like(target_interest):\n",
    "    return(user_id \n",
    "           for user_id, interest in interests\n",
    "           if user_interest == target_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_by_interest = defaultdict(list)\n",
    "interests_by_user_id = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_id, interest in interests:\n",
    "    user_ids_by_interest[interest].append(user_id)\n",
    "\n",
    "for user_id, interest in interests:\n",
    "    interests_by_user_id[user_id].append(interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0], 14)\n",
      "(['Big Data', 'Java', 'Python'], 6)\n"
     ]
    }
   ],
   "source": [
    "def max_id(dic, max_v):\n",
    "    list_id=list()\n",
    "    for id_dic in dic:\n",
    "        if len(dic[id_dic])==max_v:\n",
    "            list_id.append(id_dic)\n",
    "            max_id=id_dic   \n",
    "    return(list_id, max_v)\n",
    "\n",
    "# usu√°rio com mais interesse\n",
    "max_inte=max(len(interests_by_user_id[user_id])  for user_id in interests_by_user_id)\n",
    "print(max_id(interests_by_user_id, max_inte))\n",
    "\n",
    "# interesse mais popular\n",
    "max_1=max(len(user_ids_by_interest[interest]) for interest in user_ids_by_interest)\n",
    "print(max_id(user_ids_by_interest, max_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Salarios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {8.7: [83000],\n",
       "             8.1: [88000],\n",
       "             0.7: [48000],\n",
       "             6: [76000],\n",
       "             6.5: [69000],\n",
       "             7.5: [76000],\n",
       "             2.5: [60000],\n",
       "             10: [83000],\n",
       "             1.9: [48000],\n",
       "             4.2: [63000]})"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salaries_and_tenures = [(83000, 8.7), (88000, 8.1),\n",
    "                        (48000, 0.7), (76000, 6),\n",
    "                        (69000, 6.5), (76000, 7.5),\n",
    "                        (60000, 2.5), (83000, 10),\n",
    "                        (48000, 1.9), (63000, 4.2)]\n",
    "\n",
    "salary_by_tenure = defaultdict(list)\n",
    "\n",
    "for salary, tenure in salaries_and_tenures:\n",
    "    salary_by_tenure[tenure].append(salary)\n",
    "\n",
    "salary_by_tenure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0.7, 48000.0),\n",
       "             (1.9, 48000.0),\n",
       "             (2.5, 60000.0),\n",
       "             (4.2, 63000.0),\n",
       "             (6, 76000.0),\n",
       "             (6.5, 69000.0),\n",
       "             (7.5, 76000.0),\n",
       "             (8.1, 88000.0),\n",
       "             (8.7, 83000.0),\n",
       "             (10, 83000.0)])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_salary_by_tenure = {\n",
    "    tenure: sum(salaries) / len(salaries)\n",
    "    for tenure, salaries in salary_by_tenure.items()\n",
    "}\n",
    "\n",
    "\n",
    "average_salary_by_tenure=OrderedDict(sorted(average_salary_by_tenure.items()))\n",
    "average_salary_by_tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('2<t<5', 61500.0), ('5<', 79166.66666666667), ('<2', 48000.0)])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tenure_bucket(tenure):\n",
    "    if tenure < 2:\n",
    "        return \"<2\"\n",
    "    elif tenure < 5:\n",
    "        return \"2<t<5\"\n",
    "    else:\n",
    "        return \"5<\"\n",
    "\n",
    "salary_by_tenure_bucket = defaultdict(list)\n",
    "\n",
    "for salary, tenure in salaries_and_tenures:\n",
    "    bucket=tenure_bucket(tenure)\n",
    "    salary_by_tenure_bucket[bucket].append(salary)\n",
    "\n",
    "\n",
    "average_salary_by_tenure_bucket = {\n",
    "    tenure_bucket: sum(salaries) / len(salaries)\n",
    "    for tenure_bucket, salaries in salary_by_tenure_bucket.items()\n",
    "}\n",
    "\n",
    "average_salary_by_tenure_bucket =OrderedDict(sorted(average_salary_by_tenure_bucket .items()))\n",
    "average_salary_by_tenure_bucket\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "interests = [\n",
    "    (0, \"Hadoop\"), (0, \"Big Data\"), (0, \"HBase\"), (0, \"Java\"),\n",
    "    (0, \"Spark\"), (0, \"Storm\"), (0, \"Cassandra\"),\n",
    "    (1, \"NoSQL\"), (1, \"MongoDB\"), (1, \"Cassandra\"), (1, \"HBase\"),\n",
    "    (1, \"Postgres\"), (2, \"Python\"), (2, \"scikit-learn\"), (2, \"scipy\"),\n",
    "    (2, \"numpy\"), (2, \"statsmodels\"), (2, \"pandas\"), (3, \"R\"), (3, \"Python\"),\n",
    "    (3, \"statistics\"), (3, \"regression\"), (3, \"probability\"),\n",
    "    (4, \"machine learning\"), (4, \"regression\"), (4, \"decision trees\"),\n",
    "    (4, \"libsvm\"), (5, \"Python\"), (5, \"R\"), (5, \"Java\"), (5, \"C++\"),\n",
    "    (5, \"Haskell\"), (5, \"programming languages\"), (6, \"statistics\"),\n",
    "    (6, \"probability\"), (6, \"mathematics\"), (6, \"theory\"),\n",
    "    (7, \"machine learning\"), (7, \"scikit-learn\"), (7, \"Mahout\"),\n",
    "    (7, \"neural networks\"), (8, \"neural networks\"), (8, \"deep learning\"),\n",
    "    (8, \"Big Data\"), (8, \"artificial intelligence\"), (9, \"Hadoop\"),\n",
    "    (9, \"Java\"), (9, \"MapReduce\"), (9, \"Big Data\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big 3\n",
      "data 3\n",
      "java 3\n",
      "python 3\n",
      "learning 3\n",
      "hadoop 2\n",
      "hbase 2\n",
      "cassandra 2\n",
      "scikit-learn 2\n",
      "r 2\n",
      "statistics 2\n",
      "regression 2\n",
      "probability 2\n",
      "machine 2\n",
      "neural 2\n",
      "networks 2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Hadoop'),\n",
       " (0, 'Big Data'),\n",
       " (0, 'HBase'),\n",
       " (0, 'Java'),\n",
       " (0, 'Spark'),\n",
       " (0, 'Storm'),\n",
       " (0, 'Cassandra'),\n",
       " (1, 'NoSQL'),\n",
       " (1, 'MongoDB'),\n",
       " (1, 'Cassandra'),\n",
       " (1, 'HBase'),\n",
       " (1, 'Postgres'),\n",
       " (2, 'Python'),\n",
       " (2, 'scikit-learn'),\n",
       " (2, 'scipy'),\n",
       " (2, 'numpy'),\n",
       " (2, 'statsmodels'),\n",
       " (2, 'pandas'),\n",
       " (3, 'R'),\n",
       " (3, 'Python'),\n",
       " (3, 'statistics'),\n",
       " (3, 'regression'),\n",
       " (3, 'probability'),\n",
       " (4, 'machine learning'),\n",
       " (4, 'regression'),\n",
       " (4, 'decision trees'),\n",
       " (4, 'libsvm'),\n",
       " (5, 'Python'),\n",
       " (5, 'R'),\n",
       " (5, 'Java'),\n",
       " (5, 'C++'),\n",
       " (5, 'Haskell'),\n",
       " (5, 'programming languages'),\n",
       " (6, 'statistics'),\n",
       " (6, 'probability'),\n",
       " (6, 'mathematics'),\n",
       " (6, 'theory'),\n",
       " (7, 'machine learning'),\n",
       " (7, 'scikit-learn'),\n",
       " (7, 'Mahout'),\n",
       " (7, 'neural networks'),\n",
       " (8, 'neural networks'),\n",
       " (8, 'deep learning'),\n",
       " (8, 'Big Data'),\n",
       " (8, 'artificial intelligence'),\n",
       " (9, 'Hadoop'),\n",
       " (9, 'Java'),\n",
       " (9, 'MapReduce'),\n",
       " (9, 'Big Data')]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'big': [3],\n",
       " 'data': [3],\n",
       " 'java': [3],\n",
       " 'python': [3],\n",
       " 'learning': [3],\n",
       " 'hadoop': [2],\n",
       " 'hbase': [2],\n",
       " 'cassandra': [2],\n",
       " 'scikit-learn': [2],\n",
       " 'r': [2],\n",
       " 'statistics': [2],\n",
       " 'regression': [2],\n",
       " 'probability': [2],\n",
       " 'machine': [2],\n",
       " 'neural': [2],\n",
       " 'networks': [2],\n",
       " 'spark': [1],\n",
       " 'storm': [1],\n",
       " 'nosql': [1],\n",
       " 'mongodb': [1],\n",
       " 'postgres': [1],\n",
       " 'scipy': [1],\n",
       " 'numpy': [1],\n",
       " 'statsmodels': [1],\n",
       " 'pandas': [1],\n",
       " 'decision': [1],\n",
       " 'trees': [1],\n",
       " 'libsvm': [1],\n",
       " 'c++': [1],\n",
       " 'haskell': [1],\n",
       " 'programming': [1],\n",
       " 'languages': [1],\n",
       " 'mathematics': [1],\n",
       " 'theory': [1],\n",
       " 'mahout': [1],\n",
       " 'deep': [1],\n",
       " 'artificial': [1],\n",
       " 'intelligence': [1],\n",
       " 'mapreduce': [1]}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ids_by_word = defaultdict(list)\n",
    "interests_by_user_id = defaultdict(list)\n",
    "list_palavras= defaultdict(list)\n",
    "\n",
    "for user_id, interest in interests:\n",
    "    for word in interest.lower().split():\n",
    "        user_ids_by_word[word].append(user_id)\n",
    "\n",
    "\n",
    "max_inte=max(len(user_ids_by_word [word])  for word in user_ids_by_word )\n",
    "\n",
    "for words in user_ids_by_word:\n",
    "    count_word=len(user_ids_by_word[words])\n",
    "    user_ids_by_word[words].append(count_word)\n",
    "    index_tamanho=len(user_ids_by_word[words])-1\n",
    "    list_palavras[words].append(user_ids_by_word[words][index_tamanho])\n",
    "\n",
    "list_palavras={k: v for k, v in sorted(list_palavras.items(), key=lambda item: item[1], reverse=True)}\n",
    "list_palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
